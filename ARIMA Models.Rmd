---
title: "Evidence 2"
author: "Chantal Sim√≥ A00827554"
date: "9/6/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
      collapsed: FALSE
editor_options: 
  chunk_output_type: inline
---
```{r message=FALSE, warning=FALSE, include=FALSE}
library(stats)
library(tidyverse)  
library(ggplot2)
library(dbplyr)
library(readr)
library(stargazer)    
library(Metrics)
library(skimr)
library(xts)
library(zoo)
library(tseries)
library(stats)
library(forecast)
library(astsa)
library(funModeling)  
#__________________
library(corrplot)
library(AER)
library(vars)
library(dynlm)
library(vars)
library(mFilter)
library(tseries)
library(TSstudio)
library(lmtest)
```
# Introduction
Time series analysis is a specific way of analyzing a sequence of data points collected over an interval of time. In time series analysis, analysts record data points at consistent intervals over a set period of time rather than just recording the data points intermittently or randomly. Time series analysis can be helpful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period. This analysis is useful to forecast an outcome because it involves taking models to fit on historical data and using them to predict future observations For this problem situation we seek to forecast how many drinks would be sold and also determined in what season the most cases of soft drinks are sold 

# Background
Arca Continental is a company dedicated to the production, distribution, and sale of beverages under highly recognizable brands in Latin America and the United States. Also, is known for being the second-largest Coca-Cola bottling company in Latin America and one of the most important in the world. It serves more than 118 million consumers in Mexico, Argentina, Ecuador, United States, and Peru. 
**Financial reports:** For this problematic situation, we reviewed Arca Continental's financial reports from 2015 to 2018 where the following insights were found: The average annual increase in net profit is 21%, the sales volume of the 15% and net sales 26%.On the other hand, Assets have a constant increase while liabilities do not, due to the increase in debts in 2017 by 77% compared to the previous year. There was no debt recovery since then. However, I consider that the company as such has had good growth in general in its sales and net income, so it could be estimated that the investment made in 2017 may lead to a greater increase in profits in the future.

# Situation problem
**Problem Situation Question: In which season are the most soda soft drinks sold?**
Commonly, the sales of a company respond to cyclical and seasonal phenomena, for example, the sales of soft drinks respond to periods marked as summer, winter, etc. The seasonal and cyclical component responds to a process of demand and production. In this case, the problem situation corresponds to a predictive model of the economic-financial variable based on its components such as seasonality, cycle, trend, and randomness.

In this phase, the information generated by Arca Continental Coca-Cola is used to estimate sales based on the components that describe the time series. Considering the information generated overtime in the company, we seek to identify those factors that influence the behavior of sales, by building meaningful models to forecast and thus contribute to making strategic decisions. The ultimate goal of this problematic situation is to deliver the best time series model that offers the best predictive accuracy using various regression methods.

# Data and methodology
## Data source 
The data source used to create the models is from the "cocacolasales.csv" database where it shows by year and month the financial data and the sales behavior environment throughout 2015-2018 in a monthly basis.
```{r}
setwd("/Users/chantalasimog/Desktop/Evidence 2")
data <- read.csv("updated_cocasales_data.csv") # united data set with tsieries1 and tseries3
data2 <- read.csv("updated_cocasales_data2.csv") # tseries2 data set 
```

From this we identified the variables for the model: 

- *Varibles for Time Series*: "date", "ccsales_unit_boxes"
- *Dependent variable (Y)*: "ccsales_unit_boxes" or "sales_unitboxes"
- *Independent variable (X)*: "tperiod", "inflation_rate", "itaee_growth", "exchange_rate", "unemp_rate", "pop_density", "max_temperature", "consumer_sentiment", "gdp_percapita", "job_density", "holiday_month", "CPI", "itaee" & "pop_minwage"


## Descriptive analysis 
```{r}
# Visualize the data type and the data of each field
glimpse(data)
```
With this function we see that the type of data that are the variables. Here we have variables that are  identified as numeric or character but some of them should be fixed, for example: "date" should be date and "holiday_month" should be factor. 

```{r}
# Visualize that the data is correct and there is no NAs or NULLs
summary(data)
```
This function shows us a summary about the information of each field of the database. Its result depends on whether the data type. In the case of numeric variable it show us the Min, Max, average, median and quartiles. With the  character variables it only shows the class, length and mode.

```{r}
# Changing to factor data type this variable:
data$holiday_month <-as.factor(data$holiday_month)
str(data$holiday_month)
# Changing to date data type this variable:
data$date <- as.Date(data$date, format = "%m/%d/%Y")
str(data$date)
```
As we saw earlier, with the STR function there are some identify variables that have the wrong data type. In this case we fixed the variable "Holiday month" from numeric to factor and the variable "date from character to date. With this fixed, all the data set is clean and organize. 

```{r}
skimr::skim(data)
```
Finally, it shows us a general summary of all the content of the database. Its result depends on whether the data type: 

- In the case of the numerical variable, it shows us the NA, full rate, mean, standard deviation, quartiles and a small histogram to see the skewness 
- For character variables, it shows NAs, full rate, minimum, maximum, empty spaces, unique values, and blanks.
- For date variables it shows NAs, complete rate, minimum, maximum, median, and count of unique value 
- For factor variables it shows NAs, complete rate, order,count of unique value and top counts

```{r}
# Adding the month and year in separate columns 
data$Year <- format(data$date, "%Y")
data$Month <- format(data$date, "%m")
data$Quarter <- as.yearqtr(data$date, format = "%m/%d/%Y" )
```
In order to do some calculation we separated years and month and quartes in a seperate columns. 

## Data Vizualization 

**Time Series Plots: Sales over time**

Here we have a time series graph that shows the behavior of the company's sales from 2015 to 2018. This graph helps us to visualize how sales have been, which are the months with the highest and lowest sales, comparative annual sales status, among other things. Likewise, a line was added in the middle that presents the sales trend during this year. With this, we see if sales have increased or decreased.(This is an interactive graph)
```{r Librerias, include=FALSE}
library(dygraphs)
library(ggplot2)
library(dplyr)
library(plotly)
library(hrbrthemes)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
p <- data %>%
  ggplot( aes(x=date, y=ccsales_unit_boxes)) +
    geom_line(color="#69b3a2") +
    geom_point(color="#69b3a2") +
    theme_ipsum() +
    stat_smooth() +
    ylab("Sales") +
    xlab("Date") +
    labs(title="Sales Behavior of 2015-2018")
p <- ggplotly(p)
p
```

**Time Series Plots: Temperature over time**

Personally, I have the theory that sales also depend on the temperature for each season. Due to this, make this line graph to visualize the behavior of the temperature and compare it with the behavior of the sales to see if we find any relationship or any interesting insight.

```{r Temperature, echo=FALSE, message=FALSE, warning=FALSE}
s <- data %>%
  ggplot( aes(x=date, y=max_temperature)) +
    geom_line(color="#FFB533") +
    geom_point(color="#FFB533") +
    theme_ipsum() +
    stat_smooth() +
    ylab("Temperature") +
    xlab("Date") +
    labs(title="Temperature of 2015-2018")
s <- ggplotly(s)
s
```

**Time Series Plot: Seasonal Effect**

Lastly, we have a graph where we see the seasonal effect through the months of all the years. As the graph is of all the months from 2015 to 2018, a box plot graph was made that indicates all the measures of central tendency by month and still be able to see the general trend by months. *We also use "log" in the sales variables to reduce or eliminate skewness in our original data*. Clearly, a trend is seen within the cycle where the summer months have the highest value and the winter months the lowest.
```{r Monthly sales, echo=FALSE, message=FALSE, warning=FALSE}
b <- data %>%
  ggplot(aes(x = Month, y = log(ccsales_unit_boxes))) + 
  geom_boxplot(color = "darkmagenta") +
  theme_ipsum() +
  ylab("Sales") +
  xlab("Month") +
  labs (title = "Monthly Sales Behavior")
b <- ggplotly(b)
b
```

**Decompose Time Series Data**

The plot above shows the original time series (top), the estimated trend component (second from top), the estimated seasonal component (third from top), and the estimated irregular component (bottom). For this decomposition plot, we use the variable of "ccsales_unit_boxes" and we can observe that it has a unclear trend but a well-defined seasonal effect between 12 months. From these results we can deduce that this variable is stationary however we should run some tests to make sure if it is non-stationary

```{r Decomposition, echo=FALSE, message=FALSE, warning=FALSE}
### Decompose a time series with the ccsales_unit_boxes variable. 
datapcts<-ts(data$ccsales_unit_boxes,frequency=12,start=c(2015,1))
datapcdec<-decompose(datapcts)
plot(datapcdec)
```

**What we can infer from all this graphs?**

- From the last years the trend line on the sales show an increasing and decreasing trend.
- This trend also make us know that this variable may be stationary because there is no absolute pattern.
- Box plot across months give us a sense on seasonal effect in the months of May, June, July and August (Summer season) with their central tendency results  being higher than the rest of the months
- With the decomposition we can visualize that this variable have strong seasonal effect with a cycle of 12 months or less.
- When comparing the behavior of sales with the behavior of temperature, we see that both graphs have their peaks during May of each year. From what we could infer that the summer sessions are where it is hotter, people tend to buy more. 

## Stationary Assuption

**Main Observations**

- There is an unclear trend component which show an increasing and decreasing of the sale year by year.
- There looks to be a seasonal component which has a cycle less than 12 months.

With all these observations in mind, the next thing to check is if our variable is stationary or non-stationary and also see if this variable shows some kind of correlation. Additionally, we need to address the trend component by taking the differences in the series.

**Stationary test**

With all the observations in mind, we run some stationary test in order to see is the variables are stationary or non-stationary
```{r stationary, echo=FALSE, message=FALSE, warning=FALSE}
# Testing if the variables are stationary or not
adf.test(data$ccsales_unit_boxes)
```
Hypothesis:

- H0: if p-values > 0.05 then is Non-stationary 
- HA: if p-values < 0.05 then is Stationary 

Based on the fact that p-values has to be less than 0.05 (p-values < 0.05) we say that we reject the **null hypothesis**, meaning that this new variables are stationary enough to do any kind of time series modeling.

# Analysis of results

**Stationary Assuption Results**
As we evaluated in the Stationary tests, we obtained that our variable to evaluate is stationary. Why stationarity important in time series? When forecasting or predicting the future, most time series models assume that each point is independent of one another. The best indication of this is when the dataset of past instances is stationary. 
```{r echo=FALSE, message=FALSE, warning=FALSE}
adf.test(data$ccsales_unit_boxes)
```

**Autocorrelation & Partial autocorrelation Plot**

Autocorrelation measures the relationship between a variable‚Äôs current value and its past values. A time series can have components like trend, seasonality, cyclic and residual. ACF considers all these components while finding correlations hence its a ‚Äòcomplete auto-correlation plot‚Äô. In the other hand,partial auto-correlation function instead of finding correlations of present with lags like ACF, it finds correlation of the residuals. As a result we can see that the correlation increases and decreases, creating a wave-like trend meaning there is some meaningful correlation between is lag and the significance correlation. 

```{r echo=FALSE}
# displays the correlation among the regression residuals over the time period. 
data$ccsales_unit_boxes %>% ggtsdisplay(main="Significant Autocorrelation")
```

**Main Observations in the Correlation Plot**

- This uncover a patterns in our data with wave-like trend
- With this pattern now we select the correct forecasting methods.
- We identify an strong seasonality in our time series data.
- Now we know the appropriate ARIMA model (Seasonal Arima) for the time series prediction.

## Time Series Mdels
In this section we will discuss the observation we saw on the graphs and their behavior, then with that information we estimates based on some statistical package estimates the ARMA and ARIMA models.

### ARMA Model 
```{r ARMA}
# ARMA Model 
ARMA <- arma(log(data$ccsales_unit_boxes), order = c(1,0))
summary(ARMA)
```

### ARIMA Model 
```{r ARIMA}
# Normal ARIMA 
ARIMA <- arima(log(data$ccsales_unit_boxes), order = c(1,0,0))
summary(ARIMA)
```

### Seas.ARIMA Model 
```{r ARIMA S}
# Seasonal ARIMA (SARIMA)
ARIMA_S <- arima(log(data$ccsales_unit_boxes),order = c(1,0,0),
               seasonal=list(order=c(1, 0, 0), period=12))
summary(ARIMA_S)
```

### VAR Model
During phase A of the problem situation, we found that the best model to predict sales was with these variables: "sales_unitboxes", "itaee" "consumer_sentiment", "pop_density"and. "max_temperature". As this model was already used in phase A, it will be the same one used in this phase B. Within this model we obtained the following results:
```{r}
model<-lm(sales_unitboxes~log(exchange_rate)+ log(itaee)+ 
          log(consumer_sentiment)+ log(pop_density)+ max_temperature, data = data)
```

- p-value = 2.852e-08
- R-squared = 0.6917
- Adjusted R-squared = 0.6363 
- AIC = -130.08
- Low multicollinearity and heteroscedasticity

**Converting Variables to Time Series **

Since we know the variables with which we are going to work, now its time to convert it to time series.
```{r,results='hide'}
Sales_unitboxes <- ts(log(data$sales_unitboxes),start = c(2015,1), end = c(2018,4), frequency = 12)
Itaee <- ts(diff(data$itaee),start = c(2015,1), end = c(2018,4), frequency = 12)
Consumer_sentiment <- ts(log(data$consumer_sentiment),start = c(2015,1), end = c(2018,4), frequency = 12)
Pop_density <- ts(diff(data$pop_density),start = c(2015,1), end = c(2018,4), frequency = 12)
Max_temperature <- ts(log(data$max_temperature),start = c(2015,1), end = c(2018,4), frequency = 12)
```

```{r,results='hide'}
# We put diff for making all variables stationary
difsales<-diff(Sales_unitboxes)
difitaee<-diff(Itaee)
difconsumer<-diff(Consumer_sentiment)
difpopdensity<-diff(Pop_density)
diftemperature<-diff(Max_temperature) 
```

```{r,results='hide'}
# Now all variables are stationary
adf.test(difsales)                # p-value 0.02 -> Stationary 
adf.test(difitaee)                # p-value 0.01 -> Stationary 
adf.test(difconsumer)             # p-value 0.03 -> Stationary 
adf.test(difpopdensity)           # p-value 0.02 -> Stationary 
adf.test(diftemperature)          # p-value 0.04 -> Stationary
```

**Model Specification**
```{r}
vartseries2<-cbind(difsales,difitaee,difconsumer,difpopdensity,diftemperature)
colnames(vartseries2)<-cbind("Sales","itaee","consumer_sen","pop_density","max_temperature")
```

```{r}
lagselect2<-VARselect(vartseries2,lag.max=5,type="const")
lagselect2$selection
lagselect2$criteria
```

**VAR Model**
```{r,results='hide'}
VAR1<-VAR(vartseries2,p=1,type="const",season=NULL,exog=NULL)  
summary(VAR1)
```

```{r echo=FALSE}
variable <- c("Sales", "itaee", "consumer_sen", "pop_density", "max_temperature")
RMultiple <- c(0.3962,0.386,0.2405,0.4206,0.1171)
RAdjusted <- c(0.3019,0.2901,0.1219,0.3301,0.02083)
Pvalue <- c(0.004781,0.006037,0.1013,0.002668,0.5255)
VAR_Model <- data.frame(variable,RMultiple,RAdjusted,Pvalue)
VAR_Model 
```


## Model Diagnsostics

**Akaike Information Criterion (AIC)**
The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data. The lower the AIC the better the quality of the regression model results.
```{r}
print(-106.19) #ARMA AIC
AIC(ARIMA)
AIC(ARIMA_S)
AIC(VAR1)
```

**Residual Correlation plot**
The residual plots to check the assumptions of an OLS linear regression model. This plot help us to check and  ensure trustworthy  on our models results. 

**ACF of residuals - ARMA (1,0)**
```{r echo=FALSE, message=FALSE, warning=FALSE}
ARMA.residuals<-(ARMA$residuals)
ARMA.residuals<-na.omit(ARMA.residuals) 
checkresiduals(ARMA.residuals)
```

**ACF of residuals - ARIMA (1,0,0)**
```{r echo=FALSE, message=FALSE, warning=FALSE}
ARIMA.residuals<-(ARIMA$residuals)
ARIMA.residuals<-na.omit(ARIMA.residuals) 
checkresiduals(ARIMA.residuals)
```

**ACF of residuals - Seasonal ARIMA (1,0,0)**
```{r echo=FALSE, message=FALSE, warning=FALSE}
ARIMAS.residuals<-(ARIMA_S$residuals)
ARIMAS.residuals<-na.omit(ARIMAS.residuals) 
checkresiduals(ARIMAS.residuals)
```

**ACF of residuals - VAR Model**
```{r message=FALSE, warning=FALSE}
VAR.residual <- (model$residuals)
checkresiduals(VAR.residual,main="ACF of residuals - Model 3: VAR Model" )
```


**Ljung-Box test**
The Ljung Box test  is a way to test for the absence of serial autocorrelation, up to a specified lag k. This test uses the following hypotheses:

- H0: There is no serial correlation; Regression residual do not show temporal dependence (pv > 0.05)
- HA: There is serial correlation; Regression residual show temporal dependence (pv < 0.05)

Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.

```{r}
Box.test(ARMA.residuals,lag=1,type="Ljung-Box") # pvalue 0.9 -> Accept HO
Box.test(ARIMA.residuals,lag=1,type="Ljung-Box") # pvalue 0.8 -> Accept HO
Box.test(ARIMAS.residuals,lag=1,type="Ljung-Box") # pvalue 0.3 -> Accept HO
Box.test(VAR.residual,lag=1,type="Ljung-Box") # pvalue 0.7 -> Accept HO
```

**ADF Test** 
A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. With this, we  test if the variable is statistic or non-statistic. We see that the results are statistic because is more useful as descriptors of future behavior only if the series is stationary.
```{r message=FALSE, warning=FALSE}
#Augmented Dickey-Fuller test FOR RESIDUALS 
adf.test(ARMA.residuals) # is stationary 
adf.test(ARIMA.residuals) # is stationary
adf.test(ARIMAS.residuals) # non-stationary
adf.test(VAR.residual) # is stationary
```

## Selected Model 
```{r echo=FALSE}
# RESULTS
Model <- c('ARMA','ARIMA','ARIMA_S','VAR')
Fit <- c("(1,0)","(1,0,0)","(1,0,0)","N/A")
AIC <- c(-106.19,-102.8437,-132.821, -175.9487)
Xsquared <- c(0.0004014,0.046897,0.73242,0.064668)
BPT_pValue <- c(0.984,0.8286,0.3921,0.7993)
ADF <- c("Stationary","Stationary","Non-stationary","Stationary")
selection <- data.frame(Model,Fit,AIC,Xsquared,BPT_pValue,ADF )
selection
```

As you can see in the table above, here we have the diagnostic results of each model. AIC, Ljung-Box test, ADF Test, and Residual Correlation were tested on each and we get to the conclusion that *VAR Model** is the best one to fits for the following reasons: 

- VAR have the lowest AIC meaning this model have the better quality of the regression model results.
- VAR have be shown to be a stationary model meaning, this model is good enough to do any kind of time series modeling.
- VAR is based on the best linear regression model made in phase A. 
- Besides the others models, VARs on the Residual Correlation plot shown to have the lowest residual. 
- Lastly, the variables taken into account to perform the VAR are significant for forecasting


## Forecast
Finally here we have in a graphical way to show how the prediction of the sales in unit box. As you can see in the graph, that is the prediction for the next 10 times period. Several layers can be seen in the predicting which means how much it can vary the prediction on its highest peak and lowest peak. In general, the prediction with the highest confidence is the one in the center with a more defined color. As a results we have a low decrease in sales for the next time period.
```{r echo=FALSE, message=FALSE, warning=FALSE}
forecast<-predict(VAR1,n.ahead=10,ci=0.95) 
fanchart(forecast,names="cemex",main="Prediction of the Sales",xlab="Time Period",ylab="Sales")
```

# Conclusions 

To conclude, with all the analysis done we can draw many insides that can be of use and value for the Arca Continental company. Fore response to our problem situation, monthly data from 2015 to 2018 were obtained from Arca Continental to make a model that can have an accuracy prediction the sales in boxes of units. The data was cleaned and organized correctly and after a comprehensive diagnosis and several tests on the different model proposals, it was concluded that VAR model is the indicated model to be able to predict sales in unit boxes.

It should be noted that with this VAR model it was possible to identify the components which components of the time series variables under study are representative to explain the behavior. As a main theory, it was estimated that temperature is a variable that can strongly explain the behavior of sales in a time series (To know more see the Annex). However, with the VAR model we realized that in addition to temperature, there are other variables such as: "itaee", consumer sentiment, population density, exchange rate among other, which also affect sales.

Taking into account Arca Continental's financial statements and the model chosen to predict, it is estimated that sales and profits can continue to grow but its highly recommended to see in which season. With this model, the company will be prepared to know what are the factors , seasonality and time series that can produce an increase  or decrese in sales in unit boxes.

## Recomendation

**Recomendation with a business approach**

As mentioned above, company sales are highly dependent on time series and seasonality. As seen in the time series of sales, there is an annual growth but with a monthly pattern. As such, the observed seasonality showed that in the months with the highest temperature, the summer months, there is an increase in sales. In addition to temperature, with the VAR model we could see which are the variables that most impact sales, so in addition to temperature there are more variables that affect such as Indicator of the State Economic Activity, population density, consumer sentiment, etc. With this in mind, it is highly recommended for the company to be aware of seasonality since a great correlation could be seen between sales and seasonality.

I believe that the company can take advantage of these hot seasons to promote its soft drinks to the market and sell more. Meanwhile, on the other hand, in seasons where there are not as many sales, for example in winter, they implement marketing strategies to highlight the purchase of their drinks. You can do 2x1 promotions, bottles with names, thematic activities, DIY, among others. What is important and highly recommended for the company is that it has to take into account both the seasonality of its sales and the variances that affect it in order to grow in profits and in the industry.

# Bibliographic references
- Arcacontinental. (n.d.). CULTURA ORGANIZACIONAL. Arcacontal. Retrieved August 22, 2021, from https://www.arcacontal.com/nuestra-compa%C3%B1%C3%ADa/cultura-organizacional.aspx
- Arca Continental. (n.d.). FINANCIAL REPORTS. Arca Continental Financial-Reports. Retrieved August 26, 2021, from https://www.arcacontal.com/investors/financial-reports.aspx
- Brownlee, J. (2020, August 14). What Is Time Series Forecasting? Machine Learning Mastery. https://machinelearningmastery.com/time-series-forecasting/
- Tableau. (n.d.-b). Time Series Analysis: Definition, Types, Techniques, and When It‚Äôs Used. Retrieved September 6, 2021, from https://www.tableau.com/learn/articles/time-series-analysis#:%7E:text=Time%20series%20analysis%20is%20a,data%20points%20intermittently%20or%20randomly.
- Hayes, A., & Scott, G. (2021, April 24). Understanding Time Series. Investopedia. https://www.investopedia.com/terms/t/timeseries.asp
- Tecnol√≥gico de Monterrey. (2021). Problem Situation. Experiencia21. https://experiencia21.tec.mx/courses/203571/pages/problem-situation

# Annexe
## Temperature Theory  
My theory is that temperature is the variable that most affects sales. To verify this, perform a correlation plot where it is shown that of all the variables max_temperature is one of the most affecting sales with a correlation of approximately 0.4 to 0.6.
```{r echo=FALSE, message=FALSE, warning=FALSE}
df_num <- data[,unlist(lapply(data, is.numeric))] # We state we want all the numeric data
cor1=cor(df_num)
corrplot(cor1,method='square')
```

On the other hand, here we have both temperature and sales graphs and as you can see the highest sales peaks are on par with the highest peak temperatures for the summer season (May, June, July and August)
```{r echo=FALSE, message=FALSE, warning=FALSE}
# Comparative graph between salts and temperature
p <- data %>%
  ggplot( aes(x=date, y=ccsales_unit_boxes)) +
    geom_line(color="#69b3a2") +
    geom_point(color="#69b3a2") +
    theme_ipsum() +
    stat_smooth() +
    ylab("Sales") +
    xlab("Date") +
    labs(title="Sales Behavior of 2015-2018")
p <- ggplotly(p)
p
s <- data %>%
  ggplot( aes(x=date, y=max_temperature)) +
    geom_line(color="#FFB533") +
    geom_point(color="#FFB533") +
    theme_ipsum() +
    stat_smooth() +
    ylab("Temperature") +
    xlab("Date") +
    labs(title="Temperature of 2015-2018")
s <- ggplotly(s)
s
```

If I had to design a hypothesis I would say that:

- HO: The dependent variable of Sales is not related to the independent variable of temperature.
- HA: The dependent variable of Sales is related to the independent variable of temperature.

Although we already ran a correlation test, to check the alternative hypothesis, perform the following simple linear regression:
```{r}
summary(Modelo<-lm(sales_unitboxes~max_temperature, data = data))
```
**As results we have:**

- The explanatory level of this variable with sales is 32% which is fine to be a single independent variable.
- We also have a high level of confidence with 2.205e-05 which indicates that this variable can predict with 99% confidence.
- As an interpretation of its coefficient we have to raise the temperature one degree, sales increase $ 129,934 units, keeping everything else constant.

With all this in mind we can **reject the null hypothesis**, which means that the dependent variable of Sales is related to the independent variable of temperature.
